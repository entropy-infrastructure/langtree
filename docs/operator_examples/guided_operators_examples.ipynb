{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a16ff72",
   "metadata": {},
   "source": [
    "# Introduction to Operators\n",
    "\n",
    "In this notebook, we'll explore the basics of prompting and rendering prompts. Follow along with the code and instructions to get a hands-on experience. Feel free to modify and experiment with the code as you go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39987299-00eb-44bf-8a86-8ffc0dfe712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langtree --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f191c5-ab65-41ff-b7cd-69c3d8c43417",
   "metadata": {},
   "source": [
    "(*The above is required for compiling the docs*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e907558-b19f-46fd-9d2a-9ac94ab1f314",
   "metadata": {},
   "source": [
    "## Import the required modules\n",
    "\n",
    "For this example we will be doing a full example of operator chaining! First import the required modules.\n",
    "\n",
    "`langtree.operators`: This is where the operators for langtree will be accessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d670e1f0-985a-4793-8ac5-dc6aaca0c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langtree.models.openai import OpenAIChatCompletion\n",
    "from langtree.core import Buffer, Prompt\n",
    "from langtree.prompting import SystemMessage, UserMessage\n",
    "from langtree.operators import Parallel, Sequential, chainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ca76d-0e68-444d-9e12-aa5bb41bf854",
   "metadata": {},
   "source": [
    "## Define the model to use\n",
    "\n",
    "Here we actually define the model we will be using. Each client or adapter is a form of an `Operator`, this will be explained later (also in the api reference). For now, you can think of it like a function that happens to be able to store fixed keyword-args which are then used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10643f24-94c9-4a36-8072-5ba526e1dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = \"gpt-3.5-turbo\"\n",
    "chat = OpenAIChatCompletion(model=model1)\n",
    "\n",
    "model2 = \"gpt-4\"\n",
    "chatgpt4 = OpenAIChatCompletion(model=model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28289c-688b-4d2c-ab6d-359d9536537e",
   "metadata": {},
   "source": [
    "All our operators map 1:1 with their underlying functionality, so in this case anything passable parameters on the `openai.Completions` client are passable to our `Operator` (in this case `OpenAICompletion`)\n",
    "\n",
    "\n",
    "(What this means is that we don't necessarily *need* to have a fixed model type. We can choose to define it... or not!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17c8dc-b7fa-45f4-991f-17d854e25a23",
   "metadata": {},
   "source": [
    "## Next we build our prompts\n",
    "\n",
    "In langtree, this is super simple. Just use the `Prompt` class we imported earlier and pass a formatting string like those you'd use with langchain!\n",
    "\n",
    "Hint: Any `word` escaped by `{{ word }}` will automatically become a keyword arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79cb5edb-9d0d-4d77-93ed-2a00c4d8a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = Prompt(\"You are a helpful assistant\")\n",
    "user_prompt = Prompt(\"{{banana}}.{{dev}} is cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfa0d9-ab77-4d29-99ea-d9dd6cee769f",
   "metadata": {},
   "source": [
    "Then we can render the content of a prompt after the keywords are substituted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7e912c-389d-444a-925a-e8477acd12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_content = system_prompt()\n",
    "user_content = user_prompt(banana=\"openai\", dev=\"com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64ef43-1165-4664-adf0-dd4a46a7aa96",
   "metadata": {},
   "source": [
    "In order to use this content with a chat model, we need to convert them to Messages.\n",
    "A `Message` is essentially a dictionary with a role and content field. For usability, we've added some utility `Messages` for `System`, `User`, `Assistant`, and `Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aeee187-22eb-4fbf-9815-87a8e5687826",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SystemMessage(content=sys_content)\n",
    "u = UserMessage(content=user_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5632c5-668a-44b7-a909-771644ac4a40",
   "metadata": {},
   "source": [
    "Now we can actually use the prompts to chat! But first, we need to create some memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc245b-beec-4d83-874a-b78495b9b442",
   "metadata": {},
   "source": [
    "## Create the Memory Buffer\n",
    "\n",
    "In langtree, a `Buffer` is a simple list of fixed size. If you add more messages, it will automatically remove messages until it is the right size!\n",
    "\n",
    "Let's initialize the Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb3cb37-fca2-4b4e-9e4a-4b652be26325",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Buffer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df91a3-5472-4346-9bc9-8c9b9bbff2a5",
   "metadata": {},
   "source": [
    "Now we can add our messages to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edefa48c-cbc8-4959-8d94-4529bb48bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "history += s\n",
    "history += u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcdc5d-f7ff-41c3-a6ef-c5558d8a8bae",
   "metadata": {},
   "source": [
    "Now we are ready to chat with ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ff4eb-cf26-4995-8184-5a71810457e0",
   "metadata": {},
   "source": [
    "## Create the `@chainable` function\n",
    "\n",
    "Before, we would call the `chat` object we created. Remember, it exposes the *exact* same args as the openai completions client.\n",
    "\n",
    "Instead, we need to create a chainable function. This one is going to take history and an operator, and then return a modified version of history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e608d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chainable\n",
    "def call_func(history, op=None):\n",
    "    response = op(messages=history.memory)\n",
    "    history += UserMessage(content=response.content)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8276c7-2ea5-431a-b813-13e5d65edc9a",
   "metadata": {},
   "source": [
    "The `@chainable` decorator wraps the original function. To use a chainable function, we must first call it.\n",
    "\n",
    "**When we call a chainable function, it only takes kwargs, and stores them as static kwargs for the function. It then returns a copy of the function *WITH* the static args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1722efab-4aa2-4735-9b55-81814c53c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = call_func(op=chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7571a-af00-431a-9105-493c39b255e1",
   "metadata": {},
   "source": [
    "So now c1 has the param `op` statically set to the `chat` object we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef68ce-7287-4711-86aa-174276ce1a07",
   "metadata": {},
   "source": [
    "## Create a sequential chain\n",
    "\n",
    "To create a sequential chain, we call `Sequential` and pass a `list` of operators to it.\n",
    "\n",
    "Here we are going to initialize it as empty, and then add a chainable function that already has its static args instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "209bff8c-46df-4e6e-9bf6-8f446b825082",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential([])\n",
    "seq += c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a269d5-ad8c-4781-b347-5f6fc32dad08",
   "metadata": {},
   "source": [
    "## Add to the sequential chain\n",
    "\n",
    "To do this we can do one of two things. We can use the `+=` operator or the `sequential.add` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e058d4d-71d9-4dec-bc28-99b5e753b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq += call_func(op=chatgpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8653da3-a91a-489f-adb1-e39f13682d3f",
   "metadata": {},
   "source": [
    "## Create a parallel chain\n",
    "\n",
    "A parallel chain is almost identical to a sequential chain, except *it passes the same values to all of its operators*\n",
    "\n",
    "Additionally a parallel chain creates a deepcopy of all arguments and kwargs. This is to ensure that each operator is isolated from the others.\n",
    "\n",
    "To share objects across each function, make sure they can be passed as static kwargs when initializing the static kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4c485-869b-4f86-b6b5-19710da1b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Parallel([call_func(op=chat)])\n",
    "p += call_func(op=chat)\n",
    "p.add(call_func(op=chatgpt4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c273b-83df-4a01-93d1-d2458f1fc8c2",
   "metadata": {},
   "source": [
    "## Adding operators together\n",
    "\n",
    "We can also add sequential and parallel operators. This is totally dependent on order.\n",
    "Whichever operator type comes first in the expression is the resulting type. For example:\n",
    "\n",
    "```\n",
    "seq += p\n",
    "```\n",
    "\n",
    "Would result in adding `p` as the last operator in `seq`, and:\n",
    "\n",
    "```\n",
    "p += seq\n",
    "```\n",
    "\n",
    "Would result in adding `seq` as a parallel chain in `p`\n",
    "\n",
    "Lets see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670324e-2087-4c0e-832d-d25ee1d444d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p += seq\n",
    "\n",
    "p(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa815c",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this notebook, we covered: Importing modules, defining our endpoints, building prompts, and chatting with ChatGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
